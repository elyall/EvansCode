{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "from scipy.io import loadmat\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from distributed import Executor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# from ipywidgets import interact\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/elyall/Dropbox/Data/7737_326_001.exp\n",
      "['/Users/elyall/Dropbox/Data/7737_326_001.rois']\n"
     ]
    }
   ],
   "source": [
    "filebase = '/Users/elyall/Dropbox/Data/7737_326_001'\n",
    "# filebase = '/media/elyall/Data/7737/180118/7737_291_000' # single-depth\n",
    "ID = '7737'\n",
    "# filebase = '/media/elyall/Data/7142/170710/7142_220_002' # 4 depths\n",
    "# ID = '7142'\n",
    "# filebase = '/media/elyall/Data2/9025/181021/9025_180_002' # 4 depths\n",
    "# ID = '9025'\n",
    "# depth = [1,2,3,4];\n",
    "\n",
    "expfile = filebase + '.exp'\n",
    "fn = [filebase + '.rois']\n",
    "# fn = [filebase + \"_depth%01d\" % d + '.rois' for d in depth]\n",
    "print(expfile)\n",
    "print(fn)\n",
    "\n",
    "numX = 15      # number of neurons to test\n",
    "n_iter = 2    # number of replicates for N samples\n",
    "n_splits = 10  # cross-validation\n",
    "max_iter = 100 # regression convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9]\n",
      " [25]\n",
      " [23]\n",
      " [14]\n",
      " [21]\n",
      " [12]\n",
      " [ 7]\n",
      " [30]\n",
      " [31]\n",
      " [ 6]\n",
      " [ 2]\n",
      " [20]\n",
      " [ 8]\n",
      " [16]\n",
      " [22]\n",
      " [ 1]\n",
      " [29]\n",
      " [13]\n",
      " [17]\n",
      " [18]\n",
      " [ 0]\n",
      " [11]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [19]\n",
      " [24]\n",
      " [27]\n",
      " [15]\n",
      " [10]\n",
      " [28]\n",
      " [ 4]\n",
      " [26]\n",
      " [28]\n",
      " [ 8]\n",
      " [22]\n",
      " [24]\n",
      " [12]\n",
      " [ 6]\n",
      " [31]\n",
      " [15]\n",
      " [ 4]\n",
      " [30]\n",
      " [21]\n",
      " [ 9]\n",
      " [25]\n",
      " [26]\n",
      " [ 0]\n",
      " [20]\n",
      " [ 1]\n",
      " [19]\n",
      " [ 3]\n",
      " [14]\n",
      " [ 2]\n",
      " [16]\n",
      " [11]\n",
      " [10]\n",
      " [23]\n",
      " [29]\n",
      " [ 7]\n",
      " [27]\n",
      " [18]\n",
      " [17]\n",
      " [13]\n",
      " [ 5]\n",
      " [29]\n",
      " [15]\n",
      " [11]\n",
      " [ 0]\n",
      " [ 4]\n",
      " [25]\n",
      " [23]\n",
      " [ 3]\n",
      " [19]\n",
      " [ 9]\n",
      " [10]\n",
      " [28]\n",
      " [18]\n",
      " [27]\n",
      " [24]\n",
      " [ 6]\n",
      " [31]\n",
      " [17]\n",
      " [14]\n",
      " [21]\n",
      " [12]\n",
      " [20]\n",
      " [ 1]\n",
      " [22]\n",
      " [13]\n",
      " [30]\n",
      " [ 8]\n",
      " [26]\n",
      " [ 7]\n",
      " [ 5]\n",
      " [16]\n",
      " [ 2]\n",
      " [ 7]\n",
      " [15]\n",
      " [21]\n",
      " [24]\n",
      " [ 5]\n",
      " [29]\n",
      " [25]\n",
      " [ 6]\n",
      " [ 9]\n",
      " [18]\n",
      " [10]\n",
      " [17]\n",
      " [ 8]\n",
      " [ 0]\n",
      " [ 4]\n",
      " [26]\n",
      " [28]\n",
      " [13]\n",
      " [ 3]\n",
      " [12]\n",
      " [30]\n",
      " [31]\n",
      " [19]\n",
      " [16]\n",
      " [22]\n",
      " [ 1]\n",
      " [14]\n",
      " [11]\n",
      " [20]\n",
      " [27]\n",
      " [23]\n",
      " [ 2]\n",
      " [18]\n",
      " [20]\n",
      " [23]\n",
      " [ 2]\n",
      " [ 0]\n",
      " [ 8]\n",
      " [22]\n",
      " [ 7]\n",
      " [16]\n",
      " [ 1]\n",
      " [21]\n",
      " [27]\n",
      " [29]\n",
      " [14]\n",
      " [25]\n",
      " [ 4]\n",
      " [ 9]\n",
      " [13]\n",
      " [24]\n",
      " [15]\n",
      " [11]\n",
      " [26]\n",
      " [ 6]\n",
      " [28]\n",
      " [ 5]\n",
      " [12]\n",
      " [ 3]\n",
      " [17]\n",
      " [31]\n",
      " [30]\n",
      " [19]\n",
      " [10]\n",
      " [12]\n",
      " [14]\n",
      " [28]\n",
      " [ 5]\n",
      " [18]\n",
      " [25]\n",
      " [ 9]\n",
      " [27]\n",
      " [ 6]\n",
      " [24]\n",
      " [23]\n",
      " [29]\n",
      " [ 1]\n",
      " [ 0]\n",
      " [13]\n",
      " [20]\n",
      " [ 2]\n",
      " [ 8]\n",
      " [ 3]\n",
      " [16]\n",
      " [15]\n",
      " [ 7]\n",
      " [10]\n",
      " [19]\n",
      " [22]\n",
      " [31]\n",
      " [30]\n",
      " [17]\n",
      " [21]\n",
      " [11]\n",
      " [ 4]\n",
      " [26]\n",
      " [ 3]\n",
      " [18]\n",
      " [13]\n",
      " [12]\n",
      " [16]\n",
      " [15]\n",
      " [17]\n",
      " [ 8]\n",
      " [31]\n",
      " [30]\n",
      " [ 5]\n",
      " [28]\n",
      " [10]\n",
      " [19]\n",
      " [24]\n",
      " [ 6]\n",
      " [27]\n",
      " [ 1]\n",
      " [ 4]\n",
      " [ 9]\n",
      " [20]\n",
      " [ 7]\n",
      " [25]\n",
      " [23]\n",
      " [29]\n",
      " [22]\n",
      " [ 0]\n",
      " [21]\n",
      " [11]\n",
      " [ 2]\n",
      " [26]\n",
      " [14]\n",
      " [ 4]\n",
      " [ 2]\n",
      " [12]\n",
      " [30]\n",
      " [31]\n",
      " [28]\n",
      " [ 9]\n",
      " [19]\n",
      " [15]\n",
      " [14]\n",
      " [22]\n",
      " [13]\n",
      " [11]\n",
      " [27]\n",
      " [18]\n",
      " [ 7]\n",
      " [ 0]\n",
      " [17]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [23]\n",
      " [ 8]\n",
      " [16]\n",
      " [10]\n",
      " [ 3]\n",
      " [ 6]\n",
      " [24]\n",
      " [29]\n",
      " [20]\n",
      " [21]\n",
      " [26]\n",
      " [25]\n",
      " [20]\n",
      " [29]\n",
      " [21]\n",
      " [13]\n",
      " [ 0]\n",
      " [28]\n",
      " [11]\n",
      " [17]\n",
      " [30]\n",
      " [ 3]\n",
      " [ 7]\n",
      " [24]\n",
      " [23]\n",
      " [31]\n",
      " [14]\n",
      " [ 1]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [15]\n",
      " [16]\n",
      " [ 6]\n",
      " [18]\n",
      " [26]\n",
      " [10]\n",
      " [12]\n",
      " [ 8]\n",
      " [22]\n",
      " [19]\n",
      " [ 9]\n",
      " [ 2]\n",
      " [25]\n",
      " [27]\n",
      " [ 7]\n",
      " [28]\n",
      " [23]\n",
      " [17]\n",
      " [29]\n",
      " [12]\n",
      " [27]\n",
      " [ 6]\n",
      " [20]\n",
      " [10]\n",
      " [13]\n",
      " [30]\n",
      " [ 5]\n",
      " [18]\n",
      " [ 8]\n",
      " [11]\n",
      " [26]\n",
      " [ 2]\n",
      " [ 0]\n",
      " [22]\n",
      " [31]\n",
      " [ 9]\n",
      " [14]\n",
      " [25]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [24]\n",
      " [16]\n",
      " [19]\n",
      " [15]\n",
      " [ 1]\n",
      " [21]\n",
      " [10]\n",
      " [ 0]\n",
      " [16]\n",
      " [12]\n",
      " [31]\n",
      " [15]\n",
      " [25]\n",
      " [13]\n",
      " [ 6]\n",
      " [18]\n",
      " [29]\n",
      " [30]\n",
      " [19]\n",
      " [ 5]\n",
      " [23]\n",
      " [11]\n",
      " [ 1]\n",
      " [17]\n",
      " [28]\n",
      " [26]\n",
      " [24]\n",
      " [27]\n",
      " [22]\n",
      " [ 7]\n",
      " [ 4]\n",
      " [20]\n",
      " [ 2]\n",
      " [21]\n",
      " [ 8]\n",
      " [14]\n",
      " [ 3]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [15]\n",
      " [23]\n",
      " [ 4]\n",
      " [26]\n",
      " [ 1]\n",
      " [ 7]\n",
      " [30]\n",
      " [10]\n",
      " [12]\n",
      " [27]\n",
      " [14]\n",
      " [24]\n",
      " [ 5]\n",
      " [ 2]\n",
      " [22]\n",
      " [11]\n",
      " [ 6]\n",
      " [ 3]\n",
      " [ 0]\n",
      " [13]\n",
      " [ 8]\n",
      " [17]\n",
      " [18]\n",
      " [19]\n",
      " [20]\n",
      " [29]\n",
      " [25]\n",
      " [21]\n",
      " [16]\n",
      " [28]\n",
      " [31]\n",
      " [22]\n",
      " [ 1]\n",
      " [31]\n",
      " [21]\n",
      " [ 2]\n",
      " [ 4]\n",
      " [ 3]\n",
      " [23]\n",
      " [15]\n",
      " [ 0]\n",
      " [27]\n",
      " [17]\n",
      " [26]\n",
      " [ 7]\n",
      " [ 8]\n",
      " [25]\n",
      " [14]\n",
      " [12]\n",
      " [18]\n",
      " [ 9]\n",
      " [11]\n",
      " [29]\n",
      " [30]\n",
      " [20]\n",
      " [ 5]\n",
      " [19]\n",
      " [ 6]\n",
      " [28]\n",
      " [24]\n",
      " [13]\n",
      " [16]\n",
      " [10]\n",
      " [ 3]\n",
      " [22]\n",
      " [ 1]\n",
      " [ 7]\n",
      " [30]\n",
      " [ 9]\n",
      " [20]\n",
      " [18]\n",
      " [28]\n",
      " [ 2]\n",
      " [23]\n",
      " [ 8]\n",
      " [25]\n",
      " [10]\n",
      " [27]\n",
      " [19]\n",
      " [24]\n",
      " [17]\n",
      " [12]\n",
      " [14]\n",
      " [31]\n",
      " [13]\n",
      " [29]\n",
      " [15]\n",
      " [ 6]\n",
      " [ 5]\n",
      " [26]\n",
      " [11]\n",
      " [ 0]\n",
      " [16]\n",
      " [21]\n",
      " [ 4]\n",
      " [27]\n",
      " [22]\n",
      " [21]\n",
      " [ 2]\n",
      " [ 4]\n",
      " [ 7]\n",
      " [15]\n",
      " [24]\n",
      " [16]\n",
      " [10]\n",
      " [22]\n",
      " [ 9]\n",
      " [ 8]\n",
      " [20]\n",
      " [12]\n",
      " [30]\n",
      " [ 3]\n",
      " [17]\n",
      " [26]\n",
      " [13]\n",
      " [11]\n",
      " [ 1]\n",
      " [28]\n",
      " [25]\n",
      " [18]\n",
      " [23]\n",
      " [ 5]\n",
      " [31]\n",
      " [19]\n",
      " [10]\n",
      " [ 0]\n",
      " [ 8]\n",
      " [29]\n",
      " [ 6]\n",
      " [14]\n",
      " [17]\n",
      " [19]\n",
      " [ 0]\n",
      " [ 7]\n",
      " [ 5]\n",
      " [30]\n",
      " [26]\n",
      " [ 9]\n",
      " [ 6]\n",
      " [23]\n",
      " [27]\n",
      " [ 3]\n",
      " [11]\n",
      " [ 8]\n",
      " [31]\n",
      " [15]\n",
      " [14]\n",
      " [25]\n",
      " [ 4]\n",
      " [16]\n",
      " [13]\n",
      " [28]\n",
      " [21]\n",
      " [18]\n",
      " [29]\n",
      " [20]\n",
      " [24]\n",
      " [18]\n",
      " [ 1]\n",
      " [12]\n",
      " [ 2]\n",
      " [10]\n",
      " [22]\n",
      " [31]\n",
      " [29]\n",
      " [16]\n",
      " [ 0]\n",
      " [11]\n",
      " [ 5]\n",
      " [14]\n",
      " [23]\n",
      " [30]\n",
      " [28]\n",
      " [18]\n",
      " [18]\n",
      " [25]\n",
      " [ 1]\n",
      " [15]] <h5py._hl.dataset.AstypeContext object at 0x1a1b3cc128>\n"
     ]
    }
   ],
   "source": [
    "myfile = h5py.File(expfile,'r')\n",
    "StimID = myfile['/TrialInfo/StimID'].value[:].astype('int')\n",
    "temp = myfile['/TrialInfo/StimID'].astype('int')\n",
    "print(StimID,temp)\n",
    "myfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "457 trials with 374 neurons while randomly sampling the following neurons: [  1   2   4   5   8  13  19  30  45  69 105 160 245 374]\n"
     ]
    }
   ],
   "source": [
    "# Load stim info\n",
    "myfile = h5py.File(expfile,'r')\n",
    "StimID = myfile['/TrialInfo/StimID'][:].astype('int')\n",
    "StimLog = myfile['/Experiment/stim/stim'][:].transpose().astype('int64')\n",
    "TrialIndex = myfile['TrialIndex'][:].astype('int') - 1\n",
    "myfile.close()\n",
    "\n",
    "# Load neural data\n",
    "temp = []\n",
    "for f in fn:\n",
    "    myfile = h5py.File(f,'r')\n",
    "    numNeurons = len(myfile['ROIdata/rois/stimMean'])\n",
    "    numTrials = len(myfile['ROIdata/DataInfo/StimID'][0])\n",
    "    data = np.zeros([numTrials,numNeurons])\n",
    "    for n in np.arange(numNeurons):\n",
    "        data[:,n] = myfile[myfile['ROIdata/rois/stimMean'][n][0]][:]\n",
    "    myfile.close()\n",
    "    temp.append(data)\n",
    "data = np.concatenate(temp[:],axis=1)\n",
    "\n",
    "# keep only desired trials\n",
    "StimID = np.squeeze(StimID[TrialIndex])\n",
    "data = np.squeeze(data[TrialIndex,:])\n",
    "StimLog = StimLog[StimID,:]\n",
    "# data = np.random.random([np.size(TrialIndex),100]) # fake data\n",
    "\n",
    "numTrials, numNeurons = np.shape(data)\n",
    "# number_of_neurons = np.logspace(0,2.6,13).round().astype('int') # list of number of neurons to sample\n",
    "# number_of_neurons = np.append(number_of_neurons,numNeurons)\n",
    "number_of_neurons = np.logspace(0,np.log10(numNeurons),numX).round().astype('int') # list of number of neurons to sample\n",
    "number_of_neurons = np.unique(number_of_neurons)\n",
    "numX = len(number_of_neurons)\n",
    "print('%01d trials with %01d neurons while randomly sampling the following neurons:' % (numTrials,numNeurons), number_of_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert StimLog to vector\n",
    "a = [1,2,4,8,16]\n",
    "StimLog = StimLog.astype('int')\n",
    "for i in np.arange(StimLog.shape[1]):\n",
    "#     StimLog[:, i] *= i+1\n",
    "    StimLog[:, i] *= a[i]\n",
    "StimLog = StimLog.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(number_of_neurons):\n",
    "    \n",
    "    # initialize outputs\n",
    "    pred = np.zeros([numTrials,n_iter])\n",
    "#     pred = np.zeros([numTrials,5,n_iter])\n",
    "    perc_correct = np.zeros([n_iter])\n",
    "    \n",
    "    for ind in np.arange(n_iter):\n",
    "\n",
    "        # pull out sample of neurons\n",
    "        my_sample = np.random.choice(range(numNeurons), number_of_neurons)\n",
    "        current = data[:,my_sample]\n",
    "\n",
    "        # fit classifier to random sample (using KFold cross-validation)\n",
    "        skf = StratifiedShuffleSplit(n_splits=n_splits)\n",
    "        for train_index, test_index in skf.split(current, StimID):\n",
    "            X_train, X_test = current[train_index,:], current[test_index,:]\n",
    "            #y_train, y_test = StimID[train_index], StimID[test_index]\n",
    "            y_train, y_test = StimLog[train_index], StimLog[test_index]\n",
    "            \n",
    "            mdl = linear_model.LogisticRegression(multi_class='multinomial',max_iter=max_iter,solver='sag')\n",
    "            #mdl = RandomForestClassifier(n_estimators=100, n_jobs=-1)\n",
    "            \n",
    "            #mdl.fit(X_train, np.ravel(y_train))\n",
    "            mdl.fit(X_train, y_train)\n",
    "\n",
    "            pred[test_index,ind] = mdl.predict(X_test)\n",
    "#             pred[test_index,:,ind] = mdl.predict(X_test)\n",
    "\n",
    "        perc_correct[ind] = metrics.accuracy_score(StimID, pred[:,ind])\n",
    "#         perc_correct[ind] = metrics.accuracy_score(StimLog, pred[:,:,ind])\n",
    "\n",
    "    return(perc_correct, pred)\n",
    "\n",
    "\n",
    "# Compute classifier\n",
    "\n",
    "# # single thread\n",
    "# pred = np.zeros([numTrials,n_iter,numX])\n",
    "# # pred = np.zeros([numTrials,5,n_iter,numX])\n",
    "# perc_correct = np.zeros([n_iter,numX])\n",
    "# for n, N in enumerate(number_of_neurons):\n",
    "#     perc_correct[:,n], pred[:,:,n] = classify(N)\n",
    "# #     perc_correct[:,n], pred[:,:,:,n] = classify(N)\n",
    "\n",
    "# mult-threaded\n",
    "e = Executor()\n",
    "Futures = e.map(classify, number_of_neurons)\n",
    "out = e.gather(Futures)\n",
    "e.close()\n",
    "pred = np.zeros([numTrials,n_iter,numX])\n",
    "# pred = np.zeros([numTrials,5,n_iter,numX])\n",
    "perc_correct = np.zeros([n_iter,numX])\n",
    "for n, N in enumerate(number_of_neurons):\n",
    "    perc_correct[:,n] = out[n][0]\n",
    "    pred[:,:,n] = out[n][1]\n",
    "#     pred[:,:,:,n] = out[n][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # DOESN'T WORK BECAUSE DASK DISTRIBUTED CACHES THE RESULT OF THE FIRST ITERATION TO ALL FUTURE CALLS WITH THE SAME INPUT\n",
    "\n",
    "# def classify2(number_of_neurons):\n",
    "    \n",
    "#     # pull out sample of neurons\n",
    "#     my_sample = np.random.choice(range(numNeurons), number_of_neurons)\n",
    "#     current = data[:,my_sample]\n",
    "\n",
    "#     # fit classifier to random sample (using KFold cross-validation)\n",
    "#     pred = np.zeros([numTrials])\n",
    "#     skf = StratifiedShuffleSplit(n_splits=n_splits)\n",
    "#     for train_index, test_index in skf.split(current, StimID):\n",
    "#         X_train, X_test = current[train_index,:], current[test_index,:]\n",
    "#         y_train, y_test = StimID[train_index], StimID[test_index]\n",
    "\n",
    "# #         mdl = linear_model.LogisticRegression(max_iter=max_iter,solver='sag',multi_class='multinomial')\n",
    "#         mdl = RandomForestClassifier(n_estimators=100, n_jobs=-1)\n",
    "\n",
    "#         mdl.fit(X_train, np.ravel(y_train))\n",
    "\n",
    "#         pred[test_index] = mdl.predict(X_test)\n",
    "\n",
    "#     perc_correct = metrics.accuracy_score(StimID, pred)\n",
    "#     # confusion_matrix = metrics.confusion_matrix(StimID, pred[:,ind,n])\n",
    "\n",
    "#     return(perc_correct, pred)\n",
    "\n",
    "# # mult-threaded\n",
    "# X = np.repeat(number_of_neurons,n_iter)\n",
    "# e = Executor()\n",
    "# Futures = e.map(classify2, X)\n",
    "# out = e.gather(Futures)\n",
    "# e.close()\n",
    "# # reshape output\n",
    "# pred = np.zeros([numTrials,n_iter,numX])\n",
    "# perc_correct = np.zeros([n_iter,numX])\n",
    "# _, Nind = np.unique(X, return_inverse=True)\n",
    "# Iind = np.mod(np.arange(len(X)),n_iter)\n",
    "# for n, x, y in zip(np.arange(len(X)), Nind, Iind):\n",
    "#     perc_correct[y,x] = out[n][0]\n",
    "#     pred[:,y,x] = out[n][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import RidgeClassifierCV, LogisticRegression\n",
    "\n",
    "classifiers = {\n",
    "    'Logistic Regression' : LogisticRegression(multi_class='multinomial',max_iter=max_iter,solver='sag'),\n",
    "#     'One vs Rest' : OneVsRestClassifier(SVC(kernel='linear')),\n",
    "#     'Decision Tree' : DecisionTreeClassifier(),\n",
    "#     'Extra Tree' : ExtraTreeClassifier(),\n",
    "#     'Extra Trees' : ExtraTreesClassifier(),\n",
    "#     'K Neighbors' : KNeighborsClassifier(),\n",
    "#     'Multi-layer Perceptron' : MLPClassifier(),\n",
    "# #     'Radius Neighbors': RadiusNeighborsClassifier(radius=10.0),\n",
    "# #     'Ridge (CV)': RidgeClassifierCV(), # has formatting issue (pred is Nx1 not Nx5)\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "}\n",
    "n_classifiers = len(classifiers)\n",
    "print('%d classifier(s)' % n_classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify3(number_of_neurons):\n",
    "    \n",
    "    # initialize outputs\n",
    "    pred = np.zeros([numTrials,n_classifiers,n_iter])\n",
    "#     pred = np.zeros([numTrials,5,n_classifiers,n_iter])\n",
    "    perc_correct = np.zeros([n_classifiers,n_iter])\n",
    "    \n",
    "    for ind in np.arange(n_iter):\n",
    "\n",
    "        # pull out sample of neurons\n",
    "        my_sample = np.random.choice(range(numNeurons), number_of_neurons)\n",
    "        current = data[:,my_sample]\n",
    "\n",
    "        # perform K-fold cross validation\n",
    "        skf = StratifiedShuffleSplit(n_splits=n_splits)\n",
    "        for train_index, test_index in skf.split(current, StimID):\n",
    "            X_train, X_test = current[train_index,:], current[test_index,:]\n",
    "            y_train, y_test = StimLog[train_index], StimLog[test_index]\n",
    "#             y_train, y_test = StimLog[train_index,:], StimLog[test_index,:]\n",
    "                        \n",
    "            # fit each classifer\n",
    "            for index, (name, classifier) in enumerate(classifiers.items()):\n",
    "                classifier.fit(X_train, y_train)\n",
    "                pred[test_index,index,ind] = classifier.predict(X_test)\n",
    "#                 pred[test_index,:,index,ind] = classifier.predict(X_test)\n",
    "                perc_correct[index,ind] = metrics.accuracy_score(y_test, pred[test_index,index,ind])\n",
    "#                 perc_correct[index,ind] = metrics.accuracy_score(y_test, pred[test_index,:,index,ind])\n",
    "\n",
    "    return(perc_correct, pred)\n",
    "\n",
    "\n",
    "# Compute classifier\n",
    "\n",
    "# # single thread\n",
    "# pred = np.zeros([numTrials,5,n_classifiers,n_iter,numX])\n",
    "# perc_correct = np.zeros([n_classifiers,n_iter,numX])\n",
    "# for n, N in enumerate(number_of_neurons):\n",
    "#     perc_correct[:,:,n], pred[:,:,:,:,n] = classify3(N)\n",
    "\n",
    "# mult-threaded\n",
    "e = Executor()\n",
    "Futures = e.map(classify3, number_of_neurons)\n",
    "out = e.gather(Futures)\n",
    "e.close()\n",
    "pred = np.zeros([numTrials,n_classifiers,n_iter,numX])\n",
    "# pred = np.zeros([numTrials,5,n_classifiers,n_iter,numX])\n",
    "perc_correct = np.zeros([n_classifiers,n_iter,numX])\n",
    "for n, N in enumerate(number_of_neurons):\n",
    "    perc_correct[:,:,n] = out[n][0]\n",
    "    pred[:,:,:,n] = out[n][1]\n",
    "#     pred[:,:,:,:,n] = out[n][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save output\n",
    "h5f = h5py.File(filebase + '.h5', 'w')\n",
    "h5f.create_dataset('/perc_correct', data=perc_correct)\n",
    "h5f.create_dataset('/pred', data=pred)\n",
    "h5f.create_dataset('/number_of_neurons', data=number_of_neurons)\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot output\n",
    "X = number_of_neurons\n",
    "Y = np.mean(perc_correct,axis=1)\n",
    "E = 1.96*np.std(perc_correct,axis=1)\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(10,10))\n",
    "for ind, (y,e,l) in enumerate(zip(Y,E,list(classifiers.keys()))):\n",
    "    ax.plot(X, y, label=l)\n",
    "    ax.fill_between(X, y-e, y+e, alpha=0.1, antialiased=True)\n",
    "#     ax.errorbar(X, y, yerr=e, label=l)\n",
    "ax.set_ylabel('Percent Correct')\n",
    "ax.set_xlabel('# of Neurons')\n",
    "ax.set_xscale('log')\n",
    "ax.set_title('Mouse ' + ID)\n",
    "ax.legend(loc='upper left')\n",
    "\n",
    "# # Save plot\n",
    "# fig.savefig(ID + '_percCorrect.pdf')\n",
    "# fig.savefig(ID + '_percCorrect.eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Display single confusion matrix\n",
    "# x = np.ceil(np.sqrt(numX)).astype('int')\n",
    "# y = np.ceil(numX/x).astype('int')\n",
    "# fig, axs = plt.subplots(y,x,sharex=True,sharey=True,figsize=(10,10))\n",
    "# for ax, n in zip(axs.flat,range(numX)):\n",
    "#     confusion_matrix = metrics.confusion_matrix(np.repeat(StimID,n_iter), np.ravel(pred2[:,:,n]))\n",
    "#     ax.imshow(confusion_matrix)\n",
    "#     ax.set_title('%d' % number_of_neurons[n])\n",
    "# fig.text(0.5, 0.04, 'Predicted Label', ha='center')\n",
    "# fig.text(0.04, 0.5, 'True Label', va='center', rotation='vertical')\n",
    "    \n",
    "# fig.savefig(ID + '_RF_CM.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Display lots of confusion matrices\n",
    "# fig, ax = plt.subplots(1,1,sharex=True,sharey=True,figsize=(10,10))\n",
    "# confusion_matrix = metrics.confusion_matrix(np.repeat(StimID,n_iter), np.ravel(pred2[:,:,-1]))\n",
    "# ax.imshow(confusion_matrix)\n",
    "# ax.set_title(ID + ' RF (neurons=%d)' % number_of_neurons[-1])\n",
    "# ax.set_ylabel('True label')\n",
    "# ax.set_xlabel('Predicted label')\n",
    "\n",
    "# fig.savefig(ID + '_RF_CM_%d.png' % number_of_neurons[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
